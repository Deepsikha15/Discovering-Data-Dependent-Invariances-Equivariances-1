2022-03-08 14:36:13,547 Train loader size: 188, Validation loader size: 47, Test loader size: 40
2022-03-08 14:36:39,278 Epoch 0: Training Loss 174.33120602369308
2022-03-08 14:36:59,271 Train loader size: 188, Validation loader size: 47, Test loader size: 40
2022-03-08 14:37:13,885 Epoch 0: Training Loss 174.379078656435
2022-03-08 14:37:28,019 Epoch 1: Training Loss 59.78116700053215
2022-03-08 14:37:42,389 Epoch 2: Training Loss 56.41261404752731
2022-03-08 14:37:56,857 Epoch 3: Training Loss 52.1874285787344
2022-03-08 14:38:11,337 Epoch 4: Training Loss 49.772840678691864
2022-03-08 14:38:25,923 Epoch 5: Training Loss 46.08644438534975
2022-03-08 14:38:40,132 Epoch 6: Training Loss 41.0252977386117
2022-03-08 14:38:54,340 Epoch 7: Training Loss 36.9983981475234
2022-03-08 14:39:08,601 Epoch 8: Training Loss 33.45539442449808
2022-03-08 14:39:22,808 Epoch 9: Training Loss 30.433262214064598
2022-03-08 14:39:22,820 
RESULTS ON TEST DATA:
2022-03-08 14:39:24,886 	     precision: 0.9608
2022-03-08 14:39:24,887 	        recall: 0.9605
2022-03-08 14:39:24,887 	            F1: 0.9596
2022-03-08 14:39:24,887 	      accuracy: 0.9611
2022-03-08 14:40:07,967 Train loader size: 188, Validation loader size: 47, Test loader size: 40
2022-03-08 14:40:22,310 Epoch 0: Training Loss 174.37252061069012
2022-03-08 14:40:36,462 Epoch 1: Training Loss 59.63147850334644
2022-03-08 14:40:50,678 Epoch 2: Training Loss 56.211035296320915
2022-03-08 14:41:04,802 Epoch 3: Training Loss 51.84081169962883
2022-03-08 14:41:18,953 Epoch 4: Training Loss 49.834038734436035
2022-03-08 14:41:33,081 Epoch 5: Training Loss 47.28850084543228
2022-03-08 14:41:47,231 Epoch 6: Training Loss 41.2927011847496
2022-03-08 14:42:01,337 Epoch 7: Training Loss 37.00268394500017
2022-03-08 14:42:15,483 Epoch 8: Training Loss 33.37565930187702
2022-03-08 14:42:29,686 Epoch 9: Training Loss 30.73481795936823
2022-03-08 14:42:43,825 Epoch 10: Training Loss 27.96534239500761
2022-03-08 14:42:57,937 Epoch 11: Training Loss 26.354559425264597
2022-03-08 14:43:12,099 Epoch 12: Training Loss 25.180872555822134
2022-03-08 14:43:26,269 Epoch 13: Training Loss 23.36880349740386
2022-03-08 14:43:40,380 Epoch 14: Training Loss 23.01784286648035
2022-03-08 14:43:54,590 Epoch 15: Training Loss 22.250236954540014
2022-03-08 14:44:08,742 Epoch 16: Training Loss 21.220714822411537
2022-03-08 14:44:22,897 Epoch 17: Training Loss 20.834084786474705
2022-03-08 14:44:37,070 Epoch 18: Training Loss 20.036850318312645
2022-03-08 14:44:51,264 Epoch 19: Training Loss 20.504039451479912
2022-03-08 14:44:51,264 
RESULTS ON TEST DATA:
2022-03-08 14:44:53,288 	     precision: 0.9654
2022-03-08 14:44:53,289 	        recall: 0.9650
2022-03-08 14:44:53,289 	            F1: 0.9642
2022-03-08 14:44:53,290 	      accuracy: 0.9656
2022-03-08 14:51:12,179 Train loader size: 188, Validation loader size: 47, Test loader size: 40
2022-03-08 14:51:26,492 Epoch 0: Training Loss 174.3729150146246
2022-03-08 14:51:40,635 Epoch 1: Training Loss 59.55702328681946
2022-03-08 14:51:54,794 Epoch 2: Training Loss 56.174336567521095
2022-03-08 14:52:08,932 Epoch 3: Training Loss 51.843008518218994
2022-03-08 14:52:23,123 Epoch 4: Training Loss 51.06374427676201
2022-03-08 14:52:37,394 Epoch 5: Training Loss 47.23620603978634
2022-03-08 14:52:51,918 Epoch 6: Training Loss 41.116249568760395
2022-03-08 14:53:06,087 Epoch 7: Training Loss 37.08248561620712
2022-03-08 14:53:20,230 Epoch 8: Training Loss 33.46311169862747
2022-03-08 14:53:34,349 Epoch 9: Training Loss 30.59506382793188
2022-03-08 14:53:48,428 Epoch 10: Training Loss 27.87491111457348
2022-03-08 14:54:02,576 Epoch 11: Training Loss 26.50712823867798
2022-03-08 14:54:16,816 Epoch 12: Training Loss 25.110385484993458
2022-03-08 14:54:31,008 Epoch 13: Training Loss 23.400964181870222
2022-03-08 14:54:45,136 Epoch 14: Training Loss 22.997385416179895
2022-03-08 14:54:59,856 Epoch 15: Training Loss 22.108178053051233
2022-03-08 14:55:14,156 Epoch 16: Training Loss 21.18801924213767
2022-03-08 14:55:28,329 Epoch 17: Training Loss 20.61067247018218
2022-03-08 14:55:42,432 Epoch 18: Training Loss 20.102457374334335
2022-03-08 14:55:56,527 Epoch 19: Training Loss 20.55021859705448
2022-03-08 14:56:10,575 Epoch 20: Training Loss 19.077201407402754
2022-03-08 14:56:24,652 Epoch 21: Training Loss 19.69935017079115
2022-03-08 14:56:38,740 Epoch 22: Training Loss 19.082843890413642
2022-03-08 14:56:52,854 Epoch 23: Training Loss 18.37374721467495
2022-03-08 14:57:06,956 Epoch 24: Training Loss 18.00841299816966
2022-03-08 14:57:21,103 Epoch 25: Training Loss 17.88971932604909
2022-03-08 14:57:35,214 Epoch 26: Training Loss 19.09647774323821
2022-03-08 14:57:49,275 Epoch 27: Training Loss 18.452391367405653
2022-03-08 14:58:03,364 Epoch 28: Training Loss 17.55204301699996
2022-03-08 14:58:17,474 Epoch 29: Training Loss 16.59653233550489
2022-03-08 14:58:31,560 Epoch 30: Training Loss 16.496950866654515
2022-03-08 14:58:45,662 Epoch 31: Training Loss 16.38845770433545
2022-03-08 14:58:59,753 Epoch 32: Training Loss 16.61430487781763
2022-03-08 14:59:13,813 Epoch 33: Training Loss 25.0889183729887
2022-03-08 14:59:27,948 Epoch 34: Training Loss 20.23240091651678
2022-03-08 14:59:42,076 Epoch 35: Training Loss 19.662144223228097
2022-03-08 14:59:56,171 Epoch 36: Training Loss 17.755292212590575
2022-03-08 15:00:10,283 Epoch 37: Training Loss 16.60216074064374
2022-03-08 15:00:24,307 Epoch 38: Training Loss 16.66690074838698
2022-03-08 15:00:38,364 Epoch 39: Training Loss 16.011382738128304
2022-03-08 15:00:38,375 
RESULTS ON TEST DATA:
2022-03-08 15:00:40,410 	     precision: 0.9683
2022-03-08 15:00:40,410 	        recall: 0.9678
2022-03-08 15:00:40,411 	            F1: 0.9672
2022-03-08 15:00:40,411 	      accuracy: 0.9684
