2022-03-08 11:49:16,180 Train loader size: 188, Validation loader size: 47, Test loader size: 40
2022-03-08 11:49:46,778 Epoch 0: Training Loss 420.93871545791626
2022-03-08 11:49:46,778 saving model
2022-03-08 11:50:19,419 Epoch 1: Training Loss 283.1318708062172
2022-03-08 11:50:19,420 saving model
2022-03-08 11:50:52,229 Epoch 2: Training Loss 141.10471549630165
2022-03-08 11:50:52,229 saving model
2022-03-08 11:51:23,033 Epoch 3: Training Loss 99.0512683391571
2022-03-08 11:51:23,033 saving model
2022-03-08 11:51:53,057 Epoch 4: Training Loss 83.30113101005554
2022-03-08 11:51:53,057 saving model
2022-03-08 11:52:22,726 Epoch 5: Training Loss 75.10734847187996
2022-03-08 11:52:22,726 saving model
2022-03-08 11:52:52,431 Epoch 6: Training Loss 70.25211162865162
2022-03-08 11:52:52,431 saving model
2022-03-08 11:53:22,230 Epoch 7: Training Loss 67.00865851342678
2022-03-08 11:53:22,230 saving model
2022-03-08 11:53:51,921 Epoch 8: Training Loss 64.53131891787052
2022-03-08 11:53:51,921 saving model
2022-03-08 11:54:22,134 Epoch 9: Training Loss 62.79996506869793
2022-03-08 11:54:22,134 saving model
2022-03-08 11:54:51,734 Epoch 10: Training Loss 61.4613441079855
2022-03-08 11:54:51,734 saving model
2022-03-08 11:55:21,792 Epoch 11: Training Loss 60.23384837806225
2022-03-08 11:55:21,792 saving model
2022-03-08 11:55:53,173 Epoch 12: Training Loss 59.17551000416279
2022-03-08 11:55:53,173 saving model
2022-03-08 11:56:23,680 Epoch 13: Training Loss 58.384592205286026
2022-03-08 11:56:23,680 saving model
2022-03-08 11:56:54,371 Epoch 14: Training Loss 57.6737861931324
2022-03-08 11:56:54,371 saving model
2022-03-08 11:57:24,752 Epoch 15: Training Loss 57.08758641779423
2022-03-08 11:57:24,752 saving model
2022-03-08 11:57:54,326 Epoch 16: Training Loss 56.46753153204918
2022-03-08 11:57:54,326 saving model
2022-03-08 11:58:24,219 Epoch 17: Training Loss 55.96938098967075
2022-03-08 11:58:24,219 saving model
2022-03-08 11:58:55,928 Epoch 18: Training Loss 55.53314085304737
2022-03-08 11:58:55,928 saving model
2022-03-08 11:59:27,336 Epoch 19: Training Loss 55.13993641734123
2022-03-08 11:59:27,336 saving model
2022-03-08 11:59:58,250 Epoch 20: Training Loss 54.71402759850025
2022-03-08 11:59:58,250 saving model
2022-03-08 12:00:27,864 Epoch 21: Training Loss 54.48047460615635
2022-03-08 12:00:27,864 saving model
2022-03-08 12:00:57,861 Epoch 22: Training Loss 54.083217442035675
2022-03-08 12:00:57,861 saving model
2022-03-08 12:01:27,655 Epoch 23: Training Loss 53.91592924296856
2022-03-08 12:01:27,655 saving model
2022-03-08 12:01:57,611 Epoch 24: Training Loss 53.56868129968643
2022-03-08 12:01:57,611 saving model
2022-03-08 12:02:27,544 Epoch 25: Training Loss 53.2467410415411
2022-03-08 12:02:27,544 saving model
2022-03-08 12:02:57,373 Epoch 26: Training Loss 53.070334538817406
2022-03-08 12:02:57,373 saving model
2022-03-08 12:03:27,141 Epoch 27: Training Loss 52.83687634766102
2022-03-08 12:03:27,141 saving model
2022-03-08 12:03:57,042 Epoch 28: Training Loss 52.587032452225685
2022-03-08 12:03:57,042 saving model
2022-03-08 12:04:22,637 Epoch 29: Training Loss 52.41267742216587
2022-03-08 12:04:22,637 saving model
2022-03-08 12:04:45,488 Epoch 30: Training Loss 52.248734414577484
2022-03-08 12:04:45,488 saving model
2022-03-08 12:05:08,208 Epoch 31: Training Loss 52.16072343289852
2022-03-08 12:05:31,077 Epoch 32: Training Loss 51.878471717238426
2022-03-08 12:05:31,077 saving model
2022-03-08 12:05:53,962 Epoch 33: Training Loss 51.67480379343033
2022-03-08 12:05:53,962 saving model
2022-03-08 12:06:16,745 Epoch 34: Training Loss 51.52015659213066
2022-03-08 12:06:16,745 saving model
2022-03-08 12:06:39,499 Epoch 35: Training Loss 51.473148465156555
2022-03-08 12:06:39,499 saving model
2022-03-08 12:07:02,454 Epoch 36: Training Loss 51.367906391620636
2022-03-08 12:07:02,454 saving model
2022-03-08 12:07:25,369 Epoch 37: Training Loss 51.187768667936325
2022-03-08 12:07:25,369 saving model
2022-03-08 12:07:48,547 Epoch 38: Training Loss 51.08091475069523
2022-03-08 12:08:11,492 Epoch 39: Training Loss 50.93461400270462
2022-03-08 12:08:11,492 saving model
2022-03-08 12:08:34,185 Epoch 40: Training Loss 50.72808037698269
2022-03-08 12:08:34,185 saving model
2022-03-08 12:08:56,946 Epoch 41: Training Loss 50.6945276260376
2022-03-08 12:08:56,946 saving model
2022-03-08 12:09:21,514 Epoch 42: Training Loss 50.62539206445217
2022-03-08 12:09:21,514 saving model
2022-03-08 12:09:48,070 Epoch 43: Training Loss 50.44987988471985
2022-03-08 12:09:48,070 saving model
2022-03-08 12:10:15,248 Epoch 44: Training Loss 50.45761600136757
2022-03-08 12:10:40,493 Epoch 45: Training Loss 50.26086704432964
2022-03-08 12:10:40,493 saving model
2022-03-08 12:11:05,083 Epoch 46: Training Loss 50.209451384842396
2022-03-08 12:11:05,083 saving model
2022-03-08 12:11:29,522 Epoch 47: Training Loss 50.11504977941513
2022-03-08 12:11:29,522 saving model
2022-03-08 12:11:53,982 Epoch 48: Training Loss 49.95561645925045
2022-03-08 12:11:53,982 saving model
2022-03-08 12:12:18,393 Epoch 49: Training Loss 49.96288387477398
2022-03-08 12:12:18,393
RESULTS ON TEST DATA:
2022-03-08 12:12:21,121 	     precision: 0.9269
2022-03-08 12:12:21,121 	        recall: 0.9256
2022-03-08 12:12:21,122 	            F1: 0.9236
2022-03-08 12:12:21,122 	      accuracy: 0.9264
